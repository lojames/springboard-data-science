{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49aa4050",
   "metadata": {},
   "source": [
    "## Types of  Constraints\n",
    "\n",
    "* type\n",
    "* data range\n",
    "* uniqueness\n",
    "* membership"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bc47f",
   "metadata": {},
   "source": [
    "## str.strip(`char to remove`)\n",
    "Removes element specificed in argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3d234cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bon'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'bone'.strip('e')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fddc5f6",
   "metadata": {},
   "source": [
    "## .astype(`datatype`)\n",
    "Converts datatype of column as specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9df968",
   "metadata": {},
   "source": [
    "## Categorical Data\n",
    "\n",
    "If columns appear to be numerical, but are categorical, datatype should be `category`.  Summary statistics will be properly represented with this appropriate datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15cb9d4",
   "metadata": {},
   "source": [
    "## Converting to Pandas Datetime\n",
    "\n",
    "```\n",
    "df['date'] = pd.to_datetime(df['date'].dt.date)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6031a1",
   "metadata": {},
   "source": [
    "## Checking  and dropping duplicates\n",
    "**To check:**\n",
    "```\n",
    "duplicates = df.duplicated()\n",
    "df[duplicates].sort_values(by = col_name)\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "* `subset` list of column names to check for duplication\n",
    "* `keep` the 'first', 'last', or 'False' (all) values\n",
    "\n",
    "To drop complete duplicates (all columns match):\n",
    "```\n",
    "df.drop_duplicates(inplace=True)\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "* `subset`\n",
    "* `keep`\n",
    "* `inplace` True or False whether to drop in the working table without creating a working object\n",
    "\n",
    "To aggregate column-wise to remove duplicates:\n",
    "```\n",
    "column_names = ['a','b']\n",
    "summaries = {\n",
    "    'a': 'max',\n",
    "    'b': 'mean'\n",
    "}\n",
    "df = duplicates.groupby(by = column_names).agg(summaries).reset_index()\n",
    "\n",
    "duplicates = df.duplicated(subset = column_names, keep = False)\n",
    "df[duplicates].sort_values()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a607bf7",
   "metadata": {},
   "source": [
    "## Working with Categorical Data\n",
    "Finding the inconsistent rows\n",
    "```\n",
    "# categories is a df or series of all the correct categories for a column\n",
    "inconsistent_categories = set(df[col]).difference(categories[col])\n",
    "inconsistent_rows = df[col].isin(inconsistent_categories)\n",
    "df[inconsistent_rows]\n",
    "```\n",
    "\n",
    "Dropping inconsistent rows:\n",
    "```\n",
    "consistent_data = df[~inconsistent_rows]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c8b33",
   "metadata": {},
   "source": [
    "## Value Consistency\n",
    "\n",
    "For a series, use `.value_counts()` to look at the count of various values.\n",
    "For a dataframe, use `groupby()` with `count()`\n",
    "\n",
    "To capitalize: `str.upper()`\n",
    "To lower: `str.lower()`\n",
    "To remove leading and trailing spaces: `str.strip()`\n",
    "\n",
    "To collapse data into categories:\n",
    "```\n",
    "group_names = ['a', 'b', 'c']\n",
    "df['categories'] = pd.qcut(df[col], q = 3, labels = group_names)\n",
    "```\n",
    "```\n",
    "# Ranges specifies the cutoff points for the bins\n",
    "ranges = [0,20,50,np.inf]\n",
    "group_names = ['a','b','c']\n",
    "df['categories'] = pd.cut(df[col], bins = ranges, labels = group_names)\n",
    "```\n",
    "\n",
    "To collapse more categories into fewer, make a mapping:\n",
    "```\n",
    "mapping = {\n",
    "    'a' = 'A',\n",
    "    'b' = 'A',\n",
    "    'c' = 'A',\n",
    "    'd' = 'B',\n",
    "    'e' = 'B'\n",
    "}\n",
    "df[col] = df[col].replace(mapping)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e3928",
   "metadata": {},
   "source": [
    "## Unit Uniformity\n",
    "\n",
    "Numerical conversions can be done as follows:\n",
    "1. Subset on the values that need to be converted.\n",
    "2. Compute the conversion.\n",
    "3. Replace the subset with the converted values.\n",
    "\n",
    "Treating date:\n",
    "\n",
    "`pd.to_datetime(df[col], infer_datetime_format = True, errors = 'coerce')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596c767",
   "metadata": {},
   "source": [
    "##  Cross Field Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29b2b71",
   "metadata": {},
   "source": [
    "To compute aggregations amongst multiple columns:\n",
    "    \n",
    "`df[[col1,col2,col3]].sum(axis=1)`\n",
    "\n",
    "Useful functions for date:\n",
    "\n",
    "* `dt.date.today()`\n",
    "* `df['col'].dt.year`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2974c6",
   "metadata": {},
   "source": [
    "## Completeness\n",
    "Missing data is usually due to technical or human error.\n",
    "\n",
    "To check for missing values:\n",
    "\n",
    "`df.isna().sum()`\n",
    "\n",
    "To drop missing values:\n",
    "\n",
    "`df.dropna(subset = [col])`\n",
    "\n",
    "Replacing missing values:\n",
    "\n",
    "`df.fillna({col: value})`\n",
    "\n",
    "To visualize missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb987bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import missingno as msno\n",
    "# import matplotlib.pyplot as plt\n",
    "# msno.matrix(df)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aa3f57",
   "metadata": {},
   "source": [
    "## Minimum Edit Distance\n",
    "A systematic way to identify how close 2 strings are.  We use:\n",
    "* insert\n",
    "* delete\n",
    "* substitution\n",
    "* transpose\n",
    "\n",
    "We can use the `fuzz` package in python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11499c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from thefuzz import fuzz\n",
    "# fuzz.WRatio('Reeding', 'Reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557478d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from thefuzz import process\n",
    "# matches process.extract(string, choices, limit=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4198c",
   "metadata": {},
   "source": [
    "`matches[0]` is the matched string\n",
    "`matches[1]` is the WRatio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ded1ff",
   "metadata": {},
   "source": [
    "## Record Linkage\n",
    "Join data sources that have similar names but are actually the same entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ad78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import recordlinkage\n",
    "# indexer = recordlinkage.Index()\n",
    "# indexer.block(col)\n",
    "# pairs = indexer.index(df1,df2)\n",
    "# compare_cl = recordlinkage.Compare()\n",
    "\n",
    "## Find similar matches for columens with dates and categories\n",
    "# compare_cl.exact(col_name1, col_name2, label=col_name)\n",
    "\n",
    "## Find similar matches for columns with strings\n",
    "# compare_cl.string(col_name1, col_name1, threshold=0.85, label=col_name)\n",
    "\n",
    "## Find matches\n",
    "# potential_matches = compare_cl.compute(pairs, df1, df2)\n",
    "\n",
    "## Looking at probable matches, where n is column number\n",
    "# potential_matches[potential_matches.sum(axis = 1) >= n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3edcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting indices from 2nd df only\n",
    "# duplicate_rows = matches.index.get_level_values(1)\n",
    "\n",
    "## To get the duplicates in df2\n",
    "# df2_duplicates = df2[df2.index.isin(duplicate_rows)]\n",
    "\n",
    "## Finding non-duplicates in df2\n",
    "# df2_new = df2[~df2.index.isin(duplicate_rows)]\n",
    "\n",
    "## Linking DataFrames\n",
    "#df_linked = df1.append(df2_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
